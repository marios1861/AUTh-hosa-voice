{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prereqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "%cd /content/gdrive/MyDrive/\n",
    "%rm -rf hosa-voice\n",
    "! git clone https://github.com/marios1861/hosa-voice.git\n",
    "%cd hosa-voice\n",
    "! pip install poetry\n",
    "! poetry config virtualenvs.in-project true\n",
    "! poetry install --no-ansi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "VENV_PATH = \"/content/gdrive/MyDrive/hosa-voice/.venv/lib/python3.10/site-packages\"\n",
    "LOCAL_VENV_PATH = '/content/venv' # local notebook\n",
    "os.symlink(VENV_PATH, LOCAL_VENV_PATH) # connect to directory in drive\n",
    "sys.path.insert(0, LOCAL_VENV_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets first import and change the torchvision models' to output 4 classes instead of 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16, ViT_B_16_Weights, mobilenet_v3_large, MobileNet_V3_Large_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose which of the two models to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vit_b_16(weights=ViT_B_16_Weights.DEFAULT)\n",
    "# model = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose training lr and batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize lightning module and data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shinydemon/.cache/pypoetry/virtualenvs/hosa-voice-_kgzsXNz-py3.10/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/home/shinydemon/.cache/pypoetry/virtualenvs/hosa-voice-_kgzsXNz-py3.10/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/shinydemon/Documents/hosa-voice/scripts/a.ipynb Cell 10\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/shinydemon/Documents/hosa-voice/scripts/a.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhosa_voice\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvoice_classifier\u001b[39;00m \u001b[39mimport\u001b[39;00m VoiceClassifier\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/shinydemon/Documents/hosa-voice/scripts/a.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhosa_voice\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mvoice_datasets\u001b[39;00m \u001b[39mimport\u001b[39;00m DatasetModule\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/shinydemon/Documents/hosa-voice/scripts/a.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pl_module \u001b[39m=\u001b[39m VoiceClassifier(model, lr)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/shinydemon/Documents/hosa-voice/scripts/a.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m os\u001b[39m.\u001b[39mchdir(\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# change to root directory\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/shinydemon/Documents/hosa-voice/scripts/a.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m pl_data \u001b[39m=\u001b[39m DatasetModule(batch_size, data_dir\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdatasets/bicepstrum_image/bicepstrum_ml_normalized_imagesc_100_100\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from hosa_voice.voice_classifier import VoiceClassifier\n",
    "from hosa_voice.voice_datasets import DatasetModule\n",
    "\n",
    "pl_module = VoiceClassifier(model, lr)\n",
    "os.chdir('..') # change to root directory\n",
    "pl_data = DatasetModule(batch_size, data_dir='datasets/bicepstrum_image/bicepstrum_ml_normalized_imagesc_100_100')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "epochs = 8\n",
    "log_name = \"transformer\"\n",
    "# log_name = \"mobilenet\"\n",
    "\n",
    "\n",
    "# torch._dynamo.config.verbose=True\n",
    "trainer = Trainer(\n",
    "    # precision=\"16-mixed\",\n",
    "    # gradient_clip_algorithm=\"norm\",\n",
    "    max_epochs=epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.logger = TensorBoardLogger(\"logs\", name=log_name, sub_dir=\"tb_logs\")\n",
    "trainer.fit(pl_module, pl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.logger = CSVLogger(\"logs\", name=log_name)\n",
    "trainer.test(pl_module, pl_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hosa-voice-_kgzsXNz-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
